Before doing anything:

1. Open and follow docs/AGENT_EXECUTION_GUIDE.md as the primary governance document for this repo.
2. When the guide instructs you to consult other docs (such as:
   - docs/APP_OVERVIEW.md
   - docs/ARCHITECTURE.md
   - docs/ORCHESTRATOR_AND_AGENT_RUNTIME.md
   - docs/AI_MODELS_AND_PROVIDERS.md
   - docs/CONNECTORS_AND_INTEGRATIONS.md
   - docs/FINANCIAL_METRICS.md
   - docs/TESTING.md
   - docs/OBSERVABILITY.md
   - docs/LESSONS_LEARNED.md
   and any other .md files inside docs/),
   open and follow those sections before starting the task.

3. Treat the current contents of ALL files in the docs/ folder as the **previous baseline version** of the documentation. 
   Your job is NOT to delete or replace them, but to **update and extend** them so they fully reflect the current state of the codebase and recent enhancements.

------------------------------------------------
TASK TITLE
Global Docs Refresh & Old-vs-New Comparison (Non-Destructive Update of docs/)

TASK TYPE & PRIORITY
- Type: Documentation & Architecture Alignment
- Priority: HIGH (governance + onboarding + future development)

OVERALL GOAL
Bring the entire docs/ folder up to date with the current implementation and behavior of the app, while:

- Preserving ALL existing content (no deletions).
- Following each file’s original structure and purpose.
- Adding clearly labeled sections that:
  - Describe the **current state** of the system.
  - Highlight **enhancements vs the old version**.
  - Show, where applicable, how **performance, reliability, test coverage and maintainability** have improved.
- Ensuring the docs can serve as the **single, one-stop handover package** for future engineers, PMs and DevOps.

You must base your updates on:
- The current codebase (backend, frontend, orchestrator, connectors, tests).
- The current configuration and integrations.
- The governance and patterns already agreed in AGENT_EXECUTION_GUIDE.md and related docs.

DO NOT assume – always infer from real code and configs.

------------------------------------------------
SCOPE & DETAILED INSTRUCTIONS

A. DISCOVERY PHASE (READ-ONLY)

1) Scan docs Folder
- Enumerate all files under docs/:
  - Core governance & overview docs (e.g. AGENT_EXECUTION_GUIDE, APP_OVERVIEW, ARCHITECTURE, ORCHESTRATOR_AND_AGENT_RUNTIME, AI_MODELS_AND_PROVIDERS, CONNECTORS_AND_INTEGRATIONS, FINANCIAL_METRICS, TESTING, OBSERVABILITY, LESSONS_LEARNED).
  - Any other supporting docs (BENCHMARKING, ROADMAP_MOSCOW, AUTOMA* / AUTOMATIONS, etc.) if present.
- For each file, identify:
  - Its main purpose (governance, high-level overview, architecture, metrics, AI, connectors, testing, observability, roadmap, lessons, etc.).
  - Its current structure (headings, sections).

2) Scan Codebase & Current Behavior
- At a high level, inspect:
  - Backend modules: orchestrator, trading engine, Alpaca connector, market data connectors, DB/repository layer, risk limits, P&L logic.
  - Frontend: key screens (dashboard, analytics, strategy builder, positions/trades, settings), data fetching patterns.
  - Tests: unit, integration, E2E where present; test coverage areas.
  - Observability/logging: logger usage and any metrics or tracing.
  - AI usage: where AI models are called (decision support, explanations, Q&A, etc.), through which provider abstraction.
- Note any significant differences between what the docs claim and what the code actually does.

Do NOT change anything yet. Just understand how things actually work now.

------------------------------------------------
B. UPDATE ALL DOCS (NON-DESTRUCTIVE, VERSIONED STYLE)

General rule for ALL docs/*.md:
- DO NOT delete or rewrite existing sections wholesale.
- Only:
  - Append new sections.
  - Add clearly labeled “Current State / vNext / 2025-xx update” subsections.
  - Add “Old vs New / Improvements” subsections.
  - Make small edits where absolutely necessary (e.g. fix obviously incorrect statements), but keep original intent visible.

For every .md file in docs/, follow this pattern:

1) Preserve & Tag Existing Content (Light Touch)
- If helpful, add a small note near the top indicating:
  - That the existing sections represent the earlier baseline.
- Do NOT remove them; they serve as historical context.

2) Add a “Current State” Section
- Add a new top-level or second-level heading like:
  - “## Current State (Updated to Match Codebase)” or similar.
- Under this heading, summarise:
  - How the system behaves **now**, based on the current implementation.
  - Any key changes vs what the doc originally described.

3) Add an “Enhancements vs Previous Version” Section
- Add a heading such as:
  - “## Enhancements Compared to Previous Version”
- Describe, in bullet points or short paragraphs:
  - What has been added or improved since the original doc was written.
  - For example:
    - More robust P&L calculation and reconciliation.
    - Additional analytics metrics.
    - Improved orchestrator flow and risk checks.
    - New or refined connectors or fallbacks.
    - Stronger testing coverage, especially around trading flows.
    - Better logging and observability patterns.
    - Any AI features that are new or significantly changed.

4) Add a “Performance & Reliability Impact” Subsection (Where Relevant)
- For core docs (APP_OVERVIEW, ARCHITECTURE, ORCHESTRATOR_AND_AGENT_RUNTIME, FINANCIAL_METRICS, TESTING, OBSERVABILITY, AI_MODELS_AND_PROVIDERS, CONNECTORS_AND_INTEGRATIONS):
  - Within the enhancements section, or as a sub-section, explain how the new state has improved:
    - Performance (e.g. fewer calls, better caching, more efficient data flow).
    - Reliability and fault tolerance (e.g. fallback providers, better error handling).
    - Accuracy (e.g. P&L correctness, consistent metrics).
    - Maintainability (clearer layering, smaller modules, better tests).
  - Use concrete, code-based observations rather than vague claims.

------------------------------------------------
C. FILE-BY-FILE FOCUS AREAS

When updating, ensure you cover at least the following for each main doc (adapt to actual file names):

1) docs/AGENT_EXECUTION_GUIDE.md
- Ensure it still accurately reflects:
  - Agent modes.
  - Planning-before-coding behavior.
  - Testing/documentation expectations.
  - Lessons Learned integration.
- Add:
  - A short “Current Practices & Changes” section describing any new workflows, modes, or expectations introduced since the doc was first written.
  - A clear note that agents should always consult updated docs before major tasks.

2) docs/APP_OVERVIEW.md
- Update the “Current State” section to:
  - Reflect the present feature set (dashboard, analytics, orchestrator, integrations, AI features).
  - Mention any new capabilities introduced in the latest iteration.
- Add “Old vs New Overview” section summarizing:
  - What existed originally.
  - What has been extended or refined.
  - High-level improvements in user experience and reliability.

3) docs/ARCHITECTURE.md
- Verify the described architecture (layers, modules, flows) matches the real code.
- Update “Current State” to:
  - Show the actual architecture now (services, connectors, orchestrator, DB, UI).
- Add “Enhancements vs Previous Architecture”:
  - E.g. new market data router, improved separation of concerns, better dependency injection, etc.
- Add performance/reliability notes where architecture changes improved those aspects.

4) docs/ORCHESTRATOR_AND_AGENT_RUNTIME.md
- Ensure the document matches the actual orchestrator behavior:
  - How cycles are triggered.
  - How risk limits and kill switches work.
  - How AI decision steps and execution tie together.
- Add updated sections for:
  - Any new flows, statuses, or safeguards.
  - Any additional logging or metrics now emitted.
- Include “Old vs New Orchestrator Behavior” and “Impact on Safety & Stability”.

5) docs/AI_MODELS_AND_PROVIDERS.md
- Reflect the **actual** AI usage:
  - Which providers are wired (OpenAI/OpenRouter/others).
  - What tasks they handle (signal generation, explanations, Q&A, etc.).
- Add “Current AI Usage” and “Enhancements vs Previous AI Setup” sections:
  - Note any new models, capabilities, cost-control measures, or abstractions.

6) docs/CONNECTORS_AND_INTEGRATIONS.md
- Confirm all listed connectors are real and still in use (Alpaca, data providers, any others).
- Add “Current Integrations” describing:
  - Each connector’s responsibility.
  - Any new routing/fallback logic.
- In “Enhancements vs Previous Integrations”:
  - Highlight robustness improvements, reduced duplication, and cleaner abstractions.

7) docs/FINANCIAL_METRICS.md
- Sync metric definitions with current implementations:
  - Ensure “Not Yet Implemented” flags are accurate (or removed if implemented).
  - Adjust any examples if the underlying logic has evolved.
- Add:
  - “Current Metric Implementation Status” section summarizing which metrics are fully implemented, partially implemented, or planned.
  - “Quality & Accuracy Improvements” section citing P&L correctness or added metrics.

8) docs/TESTING.md
- Update the testing description to reflect current state:
  - What unit/integration/E2E tests exist now.
  - What frameworks and commands are used.
- Add:
  - A new section “Recent Testing Enhancements” explaining improvements in coverage around trading flows, orchestrator behavior, P&L, connectors, etc.
  - Any agreed “must-have” test scenarios that are now implemented.

9) docs/OBSERVABILITY.md
- Ensure logging, metrics, and any monitoring described are accurate.
- Add:
  - “Current Observability Stack” summarising actual loggers, tags, and any tracing/metrics.
  - “Improvements in Observability & Debuggability” (e.g. better structured logs, clearer categories).

10) docs/LESSONS_LEARNED.md
- Keep existing lessons.
- Append new lessons derived from:
  - The recent waves of changes.
  - P&L fixes, orchestrator changes, testing work, connectors, AI usage, etc.
- Ensure the structure is consistent (date, area, lesson, impact).

11) Any Other Docs (BENCHMARKING, ROADMAP_MOSCOW, AUTOMA*/AUTOMATIONS, etc.)
- Update them to:
  - Reflect the current feature set and roadmap priorities.
  - Reflect any changed assumptions after recent implementation work.
- Add short “Updated Insights” or “Current Roadmap Status” sections, without removing prior content.

------------------------------------------------
D. COMPARISON & PERFORMANCE IMPROVEMENT SUMMARY

For the main “top-level” docs (at least AGENT_EXECUTION_GUIDE, APP_OVERVIEW, ARCHITECTURE, ORCHESTRATOR_AND_AGENT_RUNTIME, AI_MODELS_AND_PROVIDERS, CONNECTORS_AND_INTEGRATIONS, FINANCIAL_METRICS, TESTING, OBSERVABILITY):

1) Add a clearly visible section in each file:
   - Title examples:
     - “Old vs New – Summary of Changes”
     - “Improvements Since Previous Version”
2) In each one:
   - Briefly compare:
     - Old state (what the doc originally described).
     - New state (current implementation).
   - Highlight improvements in:
     - Accuracy & correctness (especially financial metrics and P&L).
     - Reliability & resilience (orchestrator, connectors, error handling).
     - Performance & efficiency (fewer redundant calls, better routing, cost-aware AI).
     - Maintainability & clarity (cleaner architecture, tests, docs).
3) Keep the language factual and based on the code and tests, not assumptions.

------------------------------------------------
E. EXECUTION RULES

- Do NOT remove or fully overwrite existing sections; extend and annotate instead.
- Keep changes scoped to the docs/ folder (plus very small comment tweaks in code only if absolutely needed to clarify behavior).
- Follow the writing style and formatting patterns already used in each doc.
- Once done, ensure:
  - All updated docs are internally consistent and cross-refer to each other correctly.
  - AGENT_EXECUTION_GUIDE.md remains the primary entry point and explicitly mentions checking the updated docs.
- At the end of your work, produce a short summary message listing:
  - Which docs were updated.
  - The main categories of improvements documented (metrics, orchestrator, connectors, testing, AI, observability, etc.).
