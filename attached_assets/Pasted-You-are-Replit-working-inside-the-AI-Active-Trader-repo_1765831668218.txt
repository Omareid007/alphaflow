You are Replit working inside the AI-Active-Trader repo.

MISSION (Section 1): Make LLM usage REAL, CENTRALIZED, TRACEABLE, and POLICY-DRIVEN.
- OpenRouter is the main source for “powerful/specialized” models in CRITICAL parts.
- OpenAI smaller models OR OpenRouter free/cheap models are used for low/medium tasks.
- EVERY LLM call, decision step, tool-call, and resulting order must be recorded and linkable via a single trace_id.
- No mock data. Do NOT claim completion unless you show proof (git diff + logs + endpoints working).

NON-NEGOTIABLE RULES
1) Before coding: produce a “Reality Inventory” in the console output:
   - List all files that already implement: routing, logging, decision engine, order execution, trade ledger.
   - Specifically inspect: docs/AI_MODELS_AND_PROVIDERS.md, server/ai/roleBasedRouter.ts, server/ai/decision-engine.ts,
     server/ai/market-condition-analyzer.ts, server/ai/ai-strategy-validator.ts, shared/schema.ts, server/trading/order-execution-flow.ts.
   - For each, state: exists? used in runtime? gaps?

2) While coding:
   - Do NOT duplicate existing features if present; enrich/extend them.
   - Prefer minimal schema changes; if schema changes are needed, implement migration + db:push instructions and verify.

3) After coding: output a “Proof of Work”:
   - `git diff --stat` and list of touched files
   - `npm run check:types` result
   - show at least 2 API call examples (curl or node script) that demonstrate traceability.

========================================================
A) UNIFY ALL LLM CALLS THROUGH A SINGLE “LLM GATEWAY”
========================================================
GOAL: No direct OpenAI usage in decision-engine / analyzers. Everything goes through roleBasedRouter.

A1) Create server/ai/llmGateway.ts (or extend existing router cleanly) with ONE exported function:

  callLLM({
    role,                         // existing roles: technical_analyst, risk_manager, trade_executor, ui_assistant
    criticality,                  // "low" | "medium" | "high"
    purpose,                      // short label for logs
    traceId,                      // REQUIRED
    userId, sessionId, symbol,    // optional metadata
    system, messages, tools, toolChoice,
    responseSchema                // optional JSON Schema for strict structured output
  }) -> returns { text, json?, toolCalls?, provider, model, tokensUsed, latencyMs, traceId }

A2) Add STRUCTURED OUTPUT support at the LLM interface level:
- Update server/ai/llmClient.ts:
  - Extend LLMRequest with optional:
      responseFormat?: { type: "json_object" } OR
      responseFormat?: { type: "json_schema", json_schema: { name: string, schema: object, strict: boolean } }
- Update server/ai/openaiClient.ts and server/ai/openrouterClient.ts:
  - If req.responseFormat exists, pass it through as `response_format` in request body.

A3) Update roleBasedRouter.ts to support criticality:
- For each role config, define per-criticality model chains:
  Example (adjust to your best judgment; MUST be configurable via env overrides too):
  - technical_analyst:
      high:    primary OpenRouter deepseek/deepseek-r1 (value) -> fallback OpenRouter anthropic/claude-3.5-sonnet -> fallback OpenAI gpt-4o-mini
      medium:  primary OpenAI gpt-4o-mini -> fallback OpenRouter deepseek/deepseek-r1
      low:     primary OpenAI gpt-4o-mini (or gpt-4.1-mini if already configured) -> fallback OpenRouter free/cheap model
  - risk_manager:
      high:    primary OpenRouter anthropic/claude-3.5-sonnet -> fallback OpenRouter deepseek/deepseek-r1 -> fallback OpenAI gpt-4o-mini
      medium:  primary OpenAI gpt-4o-mini -> fallback OpenRouter anthropic/claude-3.5-sonnet
      low:     primary OpenAI gpt-4o-mini -> fallback OpenRouter cheap

- Ensure roleBasedRouter still writes to DB llm_calls (already present) but now MUST include:
  - traceId (store in metadata JSON field if you don’t want a migration, OR add a new column if you choose migration)
  - role + criticality
  - provider chosen + final model
  - purpose
  - token usage and latency (latency already computed inside clients; if missing, add it)

A4) Eliminate direct OpenAI SDK usage in:
- server/ai/decision-engine.ts
- server/ai/market-condition-analyzer.ts
- server/ai/ai-strategy-validator.ts
Replace with llmGateway.callLLM(...) with correct roles + criticality:
- decision-engine trade decisions MUST be criticality="high"
- validator/analyzer can be medium unless it blocks live trading.

========================================================
B) TRACEABILITY: ONE TRACE ID ACROSS EVERYTHING
========================================================
B1) Implement traceId generation:
- In decision-engine entrypoint that starts an “analysis run”, generate:
  traceId = crypto.randomUUID()
- Persist traceId into:
  - llm_calls records (via router)
  - ai_decisions record (add field or store in metadata JSON)
  - any “order execution attempt” record you have (or add a minimal table)

B2) Minimal DB approach (avoid heavy migrations):
Option 1 (Preferred minimal): Add a `metadata` TEXT column to llm_calls + ai_decisions + trades (JSON string)
Option 2: Add explicit columns trace_id TEXT.
Pick ONE approach and implement consistently.

B3) Create an API endpoint:
GET /api/traces/:traceId
Return a structured object:
{
  traceId,
  createdAt,
  llmCalls: [...],
  decisions: [...],
  suggestedTrades: [...],
  orderAttempts: [...],
  brokerOrders: [... if available],
  errors: [...]
}
This endpoint is mandatory to prove “everything is recorded and reusable”.

========================================================
C) ORDER TAGGING + MAPPING HOOKS (NO UI REFACTOR YET)
========================================================
C1) Ensure every broker order has a deterministic client_order_id that links to decision/trace:
- In server/trading/order-execution-flow.ts (or the call site):
  allow passing clientOrderId in the request.
- When executing a decision:
  clientOrderId = `trace_${traceId}_dec_${decisionId}` (keep under 128 chars)
- Store (decisionId, traceId, clientOrderId, alpacaOrderId, status, rawOrderJson) in DB.
If you do not want a new table, store into trades.notes/metadata as JSON.

C2) Add a lightweight “order_factors” payload:
When placing an order, attach a snapshot of:
- key features used (ai_decision_features if present)
- risk limits and why allowed
- evidence references (market snapshots ids / news ids)
Store it in DB (metadata JSON).

========================================================
D) “NO HALLUCINATION” EXECUTION GUARANTEE
========================================================
D1) Add “execution proof” checks:
- Anywhere you currently mark a trade as executed/success:
  - verify Alpaca order exists (GET by id or GET by client_order_id)
  - verify status is in {accepted,new,filled,partially_filled,...} and capture it
  - never mark “success” without broker confirmation.

D2) Remove/disable demo/mock trade generation paths in production mode:
- Search for any mock seeders or fake ledger generators.
- If needed for demo, guard with env flag DEMO_MODE=true and clearly label in UI later (NOT in this section).

========================================================
E) DOCS MUST BE UPDATED (docs folder)
========================================================
Update these docs to match the new truth (don’t invent features you didn’t implement):
- docs/AI_MODELS_AND_PROVIDERS.md
  - Explain: OpenRouter for critical powerful models, OpenAI smaller for low/medium tasks.
  - Add “criticality” concept and examples.
- docs/ORCHESTRATOR_AND_AGENT_RUNTIME.md
  - Add traceId lifecycle and how to inspect /api/traces/:traceId
- docs/OBSERVABILITY.md
  - Add where llm_calls are stored and how to debug routing choices.
If any new env vars added, list them in docs.

========================================================
F) PROOF OF WORK OUTPUT (MANDATORY)
========================================================
After implementing:
1) Show `git diff --stat`
2) Run `npm run check:types` and show success output
3) Provide 2 runnable examples:
   - Example 1: Trigger a decision run and print traceId
   - Example 2: Call GET /api/traces/:traceId and show returned structure
4) Confirm that decision-engine no longer imports/creates OpenAI client directly.

START NOW.
